# -*- coding: utf-8 -*-
"""Murilo_Holtz_IA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QaIJh07BPz6B41tV4CaFBwkQl2Xr4r-x

> #### Murilo Holtz Foltran 133770

```
projeto de Inteligência Artificial pela Unifesp
Profª Drª Lilian Berton
detectar câncer de mama através de uma base de dado
detectar = classificação por machine learning
```
"""

pip install minisom

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from google.colab import files
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler
from minisom import MiniSom
from sklearn.preprocessing import StandardScaler
from pylab import pcolor, colorbar
from sklearn.model_selection import train_test_split

arq = files.upload() # base de dado a ser upada
path = list(arq.keys())[0] # pegando o nome
df = pd.read_csv(path) # lendo em um dataframe
df.head()

# shape: formato dos dados (linhas: populacao; colunas: info)
populacao, info = df.shape[0], df.shape[1]
print('Populaçao: {}; Colunas: {}'.format(populacao, info))

# contanto os valores vazios (empty values: NaN, NAN, na, '')
emptyValues = df.isna()
# display(emptyValues)
display(emptyValues.sum())

# Unnamed: 32 - coluna com 569 valores vazios
df = df.dropna(axis=1)

df.head()

print('{}\n'.format(df['diagnosis'].value_counts()))
# visualizando: 
sns.set_theme(style="darkgrid", palette='Set1', color_codes=True)
sns.set(rc={'figure.figsize':(8,6)})
graphDiagnosis = sns.countplot(x = df['diagnosis'])

df.dtypes

# trocando M e B por 1 e 0, para melhor criaçao e analise dos dados
df.iloc[:,1] = LabelEncoder().fit_transform(df.iloc[:,1].values)
print(LabelEncoder().fit_transform(df.iloc[:,1].values))

sns.pairplot(df.iloc[:,1:7], hue="diagnosis")

# azul: benigno
# laranja: maligno

df.head(7)

# correlações entre colunas
df.iloc[:,1:12].corr() # ignorando id

plt.figure(figsize=(13, 10))
sns.heatmap(df.iloc[:,1:12].corr(), annot=True, fmt='.0%') # mapa de calor

# criando dados independentes e dependentes
X = df.iloc[:,2:31].values
Y = df.iloc[:,1].values

X

"""# mapas auto-organizáveis"""

normalizador = MinMaxScaler(feature_range = (0, 1))
Xn = normalizador.fit_transform(X)

Xn

som = MiniSom(x=11, y=11, input_len = 29,
              learning_rate = 0.5, 
              random_seed = 2)

som.random_weights_init(Xn)
som.train_random(data = Xn, num_iteration = 100)

# som._weights
# som._activation_map
q = som.activation_response(Xn)
q

pcolor(som.distance_map().T)
colorbar()

pcolor(som.distance_map().T)
colorbar()
w = som.winner(Xn[0]) # neuronio vencedor de cada registro (similiaridade)
                        # winner / best matching unit 
markers = ['o', 's']
color = ['g', 'r']

for i, x in enumerate(Xn):
    #print(i)
    #print(x)
    w = som.winner(x)
    #print(w)
    plt.plot(w[0] + 0.5, w[1] + 0.5, markers[Y[i]],
         markerfacecolor='None', markersize = 10,
         markeredgecolor = color[Y[i]], markeredgewidth = 2)

"""# classificação"""

# treinamento e teste
# 75% e 25%
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, 
                                                    test_size=0.25, 
                                                    random_state=0)

# feature scaling / normalizar o intervalo de variáveis ​​independentes
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.fit_transform(X_test)

# funcao para os modelos
def algdeclass(x, y):
    #regressao logistica
    from sklearn.linear_model import LogisticRegression
    lr = LogisticRegression(random_state=0)
    lr.fit(x, y)

    #k-nearest neighbours
    from sklearn.neighbors import KNeighborsClassifier
    knn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
    knn.fit(X_train, Y_train)

    #arvore de decisao
    from sklearn.tree import DecisionTreeClassifier
    tree = DecisionTreeClassifier(criterion = 'entropy', random_state=0)
    tree.fit(x, y)

    #floresta aleatoria
    from sklearn.ensemble import RandomForestClassifier
    forest = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state=0)
    forest.fit(x, y)

    return lr, knn, tree, forest

modelos = ['Regressão Logística', 'KNN', 'Árvore de decisão', 'Floresta Aleatória']

algdeclass = algdeclass(X_train, Y_train)

# matriz de confusão: testando acurácia do modelo nos dados de teste
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(Y_test, algdeclass[0].predict(X_test))

positivo = cm[0][0]
negativo = cm[1][1]
falsoNegativo = cm[1][0]
falsoPositivo = cm[0][1]

print('Matriz de confusão:')
print(cm)
# positivos, falso positivo
# falso negativo, negativo

print('\nAcurácia do teste:\n', (positivo + negativo)/(positivo 
                                                    + negativo 
                                                    + falsoNegativo 
                                                    + falsoPositivo))

for i in range(len(algdeclass)):
    print('modelo', i)
    print(modelos[i])
    print('')
    cm = confusion_matrix(Y_test, algdeclass[i].predict(X_test))
    positivo = cm[0][0]
    negativo = cm[1][1]
    falsoNegativo = cm[1][0]
    falsoPositivo = cm[0][1]
    print(cm)

    print('Acurácia do teste: ', (positivo + negativo)/(positivo
                                                        + negativo
                                                        + falsoNegativo 
                                                        + falsoPositivo))
    print('')

from sklearn.metrics import classification_report 
from sklearn.metrics import accuracy_score
for i in range(len(algdeclass)):
    print('modelo: ', modelos[i])
    print('Classification Report:\n ',classification_report(Y_test, algdeclass[i].predict(X_test)))
    print('Acurácia: ', accuracy_score(Y_test, algdeclass[i].predict(X_test)))
    print('')

predtrain = algdeclass[3].predict(X_train)
print(predtrain)
print('')
print(Y_train)

plt.figure(figsize=(25, 5))
pcolor([predtrain, Y_train])

predtest = algdeclass[3].predict(X_test)
print(predtest)
print('')
print(Y_test)

plt.figure(figsize=(25, 5))
pcolor([predtest, Y_test])

predtest2 = algdeclass[2].predict(X_test)
print(predtest2)
print('')
print(Y_test)

plt.figure(figsize=(14, 10))
pcolor([predtest2, Y_test])

